{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to generate the livecell data, and pretrain models on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import ast\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from utils.plots import *\n",
    "from utils.logger import prepare_log_folder, create_logger, save_config\n",
    "\n",
    "from data.preparation import prepare_extra_data\n",
    "from data.dataset import SartoriusDataset\n",
    "from data.transforms import define_pipelines\n",
    "\n",
    "from main_training import BATCH_SIZES\n",
    "from training.main import pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = glob.glob(DATA_PATH + \"LIVECell_dataset_2021/annotations/LIVECell_single_cells/*/*.json\")\n",
    "annotations = []  # do not recompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SHSY5Y_ONLY = False\n",
    "NO_SHSY5Y = True\n",
    "SINGLE_CLASS = False\n",
    "\n",
    "name = \"livecell.csv\"\n",
    "classes = LIVECELL_CLASSES\n",
    "\n",
    "if SHSY5Y_ONLY:\n",
    "    annotations = [a for a in annotations if \"shsy5y\" in a]\n",
    "    name = \"livecell_shsy5y.csv\"\n",
    "elif NO_SHSY5Y:\n",
    "    annotations = [a for a in annotations if \"shsy5y\" not in a]\n",
    "    name = \"livecell_no_shsy5y.csv\"\n",
    "    classes = ['', '', ''] + LIVECELL_CLASSES[:-1]\n",
    "\n",
    "if SINGLE_CLASS:\n",
    "    assert NO_SHSY5Y\n",
    "    name = \"livecell_no_shsy5y_single.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metas = []\n",
    "for path in tqdm(annotations):\n",
    "    filename = path.split('/')[-1]\n",
    "    _, cell_type, split = filename.split('.')[0].split('_')\n",
    "    print(f\"\\n -> Processing {cell_type}_{split}\")\n",
    "    annots = json.load(open(path, 'r'))\n",
    "    \n",
    "    annots[\"annotations\"] = list(annots[\"annotations\"].values())\n",
    "    coco = COCO()\n",
    "    coco.dataset = annots\n",
    "    coco.createIndex()\n",
    "\n",
    "    cell_index = -1 if SHSY5Y_ONLY else classes.index(cell_type.lower())\n",
    "    cell_index = 3 if SINGLE_CLASS else cell_index\n",
    "    \n",
    "    for image in annots['images']:\n",
    "        \n",
    "        boxes, rles = [], []\n",
    "        for annot in coco.anns.values():\n",
    "            if annot['image_id'] == image['id']:\n",
    "                rles.append(coco.annToRLE(annot))\n",
    "                \n",
    "                box = np.array(annot[\"bbox\"])\n",
    "                box[2] += box[0]\n",
    "                box[3] += box[1]\n",
    "                boxes.append(box)\n",
    "        \n",
    "        meta = {\n",
    "            'filename': image['file_name'],\n",
    "            'width': image['width'],\n",
    "            'height': image['height'],\n",
    "            'cell_type': cell_type,\n",
    "            'split': split,\n",
    "            'ann': {\n",
    "                'bboxes': np.array(boxes).astype(int).tolist(),\n",
    "                'labels': [cell_index] * len(boxes),\n",
    "                'masks': rles\n",
    "            }\n",
    "        }\n",
    "        metas.append(meta)\n",
    "        \n",
    "#     break\n",
    "\n",
    "if len(metas):\n",
    "    meta_df = pd.DataFrame.from_dict(metas)\n",
    "    meta_df.to_csv(OUT_PATH + name, index=False)\n",
    "    \n",
    "    print(f' -> Saved to \"{OUT_PATH + name}\"')\n",
    "    \n",
    "    sns.countplot(x=meta_df['cell_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Parameters used for training\n",
    "    \"\"\"\n",
    "    # Images\n",
    "    use_mosaic = False\n",
    "    use_tta = False  # TODO\n",
    "    data_config = \"configs/config_aug_mosaic.py\" if use_mosaic else \"configs/config_aug.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = prepare_extra_data(name=\"livecell\")\n",
    "df = prepare_extra_data(name=\"livecell_shsy5y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipelines = define_pipelines(Config.data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SartoriusDataset(df, pipelines['val_viz'], precompute_masks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in range(10):\n",
    "    idx = np.random.choice(len(dataset))\n",
    "\n",
    "    data = dataset[idx]\n",
    "\n",
    "    img = data['img']\n",
    "    boxes = data['gt_bboxes']\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plot_sample(img, data['gt_masks'], plotly=False)\n",
    "    plt.title(df['cell_folder'][idx])\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Parameters used for training\n",
    "    \"\"\"\n",
    "    # General\n",
    "    seed = 42\n",
    "    verbose = 1\n",
    "    first_epoch_eval = 10\n",
    "    compute_val_loss = False\n",
    "    verbose_eval = 5\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    save_weights = True\n",
    "\n",
    "    # Images\n",
    "    fix = True\n",
    "    extra_name = \"\"\n",
    "    use_extra_samples = False\n",
    "    num_classes = 8\n",
    "\n",
    "    use_mosaic = False\n",
    "    use_pl = False\n",
    "    data_config = \"configs/config_aug_mosaic.py\" if use_mosaic else \"configs/config_aug.py\"\n",
    "\n",
    "    # k-fold\n",
    "    k = 50\n",
    "    random_state = 0\n",
    "    selected_folds = [0]\n",
    "\n",
    "    # Model\n",
    "    name = \"cascade\"  # \"cascade\"\n",
    "    encoder = \"resnext50\"\n",
    "    model_config = f\"configs/config_{name}.py\"\n",
    "    pretrained_livecell = False\n",
    "    \n",
    "    if name == \"htc\":\n",
    "        data_config = \"configs/config_aug_semantic.py\"\n",
    "\n",
    "    # Training\n",
    "    optimizer = \"AdamW\"  # \"Adam\"\n",
    "    scheduler = \"linear\"\n",
    "    weight_decay = 0.01  # \"0\"\n",
    "    batch_size = BATCH_SIZES[name][encoder]\n",
    "    val_bs = batch_size\n",
    "    freeze_bn = batch_size < 3\n",
    "    loss_decay = False\n",
    "\n",
    "    epochs = 2 * batch_size\n",
    "\n",
    "    lr = 3e-4\n",
    "    warmup_prop= 0.05\n",
    "\n",
    "    use_fp16 = False  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "log_folder = None\n",
    "LOG_PATH = \"../logs/pretrain/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not DEBUG:\n",
    "    log_folder = prepare_log_folder(LOG_PATH)\n",
    "    print(f\"Logging results to {log_folder}\\n\")\n",
    "    save_config(Config, log_folder)\n",
    "    create_logger(directory=log_folder, name=\"logs.txt\")\n",
    "\n",
    "results = pretrain(Config, log_folder=log_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
