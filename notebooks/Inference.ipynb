{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to perform inference on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from utils.plots import *\n",
    "from utils.metrics import *\n",
    "from utils.logger import Config\n",
    "from utils.rle import rle_encode, rle_decode\n",
    "\n",
    "from inference.tweaking import *\n",
    "from inference.validation import *\n",
    "from inference.post_process import *\n",
    "\n",
    "from data.preparation import prepare_data\n",
    "from data.dataset import SartoriusDataset\n",
    "from data.transforms import define_pipelines\n",
    "from inference.validation import inference_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # 2nd ensemble - CV 0.3207 - LB 0.337\n",
    "    LOG_PATH + \"2021-11-15/3/\",  # 7.  rx101 pretrain + extra - 0.3153\n",
    "    LOG_PATH + \"2021-11-16/0/\",  # 8.  r50 pretrain + extra - 0.3135\n",
    "    LOG_PATH + \"2021-11-16/3/\",  # 9.  rx101 pretrain  - 0.3155\n",
    "    LOG_PATH + \"2021-11-17/0/\",  # 10. r50 pretrain - 0.3127\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # 2.5 ensemble - CV 0.3221 - LB ?\n",
    "    LOG_PATH + \"2021-11-16/0/\",  # 8.  r50 pretrain + extra - 0.3135\n",
    "    LOG_PATH + \"2021-11-16/3/\",  # 9.  rx101 pretrain  - 0.3155\n",
    "    LOG_PATH + \"2021-11-22/1/\",  # 12. Cascade r50 - 0.3139\n",
    "    LOG_PATH + \"2021-11-22/5/\",  # 13. Cascade rx101 - 0.3161\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  \n",
    "#     LOG_PATH + \"2021-11-16/0/\",  # r50 pretrain + extra - 0.3135\n",
    "#     LOG_PATH + \"2021-11-16/3/\",  # rx101 pretrain  - 0.3155\n",
    "#     LOG_PATH + \"2021-11-22/1/\",  # Cascade r50 - 0.3139\n",
    "#     LOG_PATH + \"2021-11-22/5/\",  # Cascade rx101 - 0.3161   (0.3207)\n",
    "#     LOG_PATH + \"2021-11-24/0/\",  # Cascade rx101 stricter rpn - 0.3151\n",
    "#     LOG_PATH + \"2021-11-25/1/\",  # htc rx101 - 0.3141\n",
    "\n",
    "    LOG_PATH + \"2021-11-25/6/\",  # Cascade rx101 AdamW 2e-4  - 0.3162\n",
    "    LOG_PATH + \"2021-11-26/0/\",  # Cascade r50 AdamW 2e-4 30ep - 0.3149\n",
    "#     LOG_PATH + \"2021-11-26/1/\",  # Cascade swin_s\n",
    "#     LOG_PATH + \"2021-11-27/1/\",  # Cascade mask scoring - TODO : ensemble wrapper\n",
    "    LOG_PATH + \"2021-11-28/1/\",  # Cascade r101_64x4\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # 3rd ensemble - CV 0.3226\n",
    "    LOG_PATH + \"2021-11-16/3/\",  # 9.  rx101 pretrain  - 0.3155\n",
    "    LOG_PATH + \"2021-11-22/5/\",  # 13. Cascade rx101 - 0.3161\n",
    "    LOG_PATH + \"2021-11-25/6/\",  # 16. Cascade rx101   - 0.3162\n",
    "    LOG_PATH + \"2021-11-26/0/\",  # 17. Cascade r50     - 0.3149\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [  # 5th ensemble - new CV\n",
    "    LOG_PATH + \"2021-11-29/4/\",  # 20. Cascade rx101 - 0.3170\n",
    "    LOG_PATH + \"2021-11-30/2/\",  # 21. Cascade r50   - 0.3162\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TTA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs, weights = [], []\n",
    "\n",
    "for exp_folder in EXP_FOLDERS:\n",
    "    config = Config(json.load(open(exp_folder + \"config.json\", 'r')))\n",
    "\n",
    "    config.model_config = exp_folder + config.model_config.split('/')[-1]\n",
    "    config.data_config = exp_folder + config.data_config.split('/')[-1]\n",
    "\n",
    "    try:\n",
    "        _ = config.split\n",
    "        remove_anomalies = config.remove_anomalies\n",
    "    except:\n",
    "        config.split = \"skf\"\n",
    "        remove_anomalies = False\n",
    "\n",
    "    configs.append(config)\n",
    "    \n",
    "    weights.append(sorted(glob.glob(exp_folder + \"*.pt\")))\n",
    "#     weights.append(sorted(glob.glob(exp_folder + \"*.pt\"))[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(fix=False, remove_anomalies=remove_anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "all_results, dfs_val = inference_val(df, configs, weights, use_tta=USE_TTA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof = pd.concat(dfs_val).reset_index(drop=True)\n",
    "pipelines = define_pipelines(config.data_config)\n",
    "\n",
    "datasets = [SartoriusDataset(df_val, transforms=pipelines['val_viz'], precompute_masks=False) for df_val in dfs_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds_mask = [0.45, 0.45, 0.45]\n",
    "best_thresholds_nms = [0.1, 0.1, 0.05]\n",
    "best_thresholds_conf = [0.3, 0.4, 0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds_mask = [0.45]\n",
    "\n",
    "thresholds_nms = [np.round(0.05 * i, 2) for i in range(1, 5)]\n",
    "\n",
    "thresholds_conf = [np.round(0.05 * i, 2) for i in range(4, 17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "for dataset, results in zip(datasets, all_results):\n",
    "    scores = tweak_thresholds(\n",
    "        results,\n",
    "        dataset,\n",
    "        thresholds_mask,\n",
    "        thresholds_nms,\n",
    "        thresholds_conf,\n",
    "        remove_overlap=True,\n",
    "        corrupt=True,\n",
    "    )\n",
    "    all_scores.append(scores)\n",
    "#     break\n",
    "\n",
    "scores_tweak = [\n",
    "    np.concatenate([scores_fold[c] for scores_fold in all_scores], 2)\n",
    "    for c in range(len(CELL_TYPES))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores = []\n",
    "\n",
    "for c in range(len(CELL_TYPES)):\n",
    "    print(f' -> Cell type {CELL_TYPES[c]} : ')\n",
    "\n",
    "    scores_class = scores_tweak[c].mean(2) \n",
    "    idx = np.unravel_index(np.argmax(scores_class, axis=None), scores_class.shape)\n",
    "    best_score = scores_class[idx]\n",
    "    best_scores.append(best_score)\n",
    "\n",
    "    best_thresholds_c = (thresholds_mask[idx[0]], thresholds_nms[idx[1]], thresholds_conf[idx[2]])\n",
    "    best_thresholds_mask[c] = best_thresholds_c[0]\n",
    "    best_thresholds_nms[c] = best_thresholds_c[1]\n",
    "    best_thresholds_conf[c] = best_thresholds_c[2]\n",
    "\n",
    "    print(f\"Best score {best_score:.4f} for thresholds (mask, nms, conf): {best_thresholds_c}\\n\")\n",
    "\n",
    "best_score = np.average(best_scores, weights=[Counter(df_oof['cell_type'])[c] for c in CELL_TYPES])\n",
    "print(f'CV score : {best_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for c in range(len(CELL_TYPES)):\n",
    "#     print(f\"\\nClass {CELL_TYPES[c]}\")\n",
    "#     for idx_mask, threshold_mask in enumerate(thresholds_mask):\n",
    "#         for idx_nms, threshold_nms in enumerate(thresholds_nms):\n",
    "#             print(f\"\\n-> Threshold mask = {threshold_mask} - Threshold nms = {threshold_nms}\")\n",
    "#             try:\n",
    "#                 for s, conf in zip(np.mean(scores_tweak[c][idx_mask, idx_nms], 0) , thresholds_conf):\n",
    "#                     print(f\"Threshold conf = {conf} - score = {s:.4f}\")\n",
    "#             except:\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'THRESHOLDS_MASK = {best_thresholds_mask}')\n",
    "print(f'THRESHOLDS_NMS = {best_thresholds_nms}')\n",
    "print(f'THRESHOLDS_CONF = {best_thresholds_conf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = [[], [], []]\n",
    "metadata = []\n",
    "\n",
    "for results, dataset in zip(all_results, datasets):\n",
    "    masks_pred, boxes_pred, cell_types = process_results(\n",
    "        results, best_thresholds_mask, best_thresholds_nms, best_thresholds_conf, remove_overlap=True\n",
    "    )\n",
    "    \n",
    "    scores, scores_per_class = evaluate(\n",
    "        masks_pred,\n",
    "        dataset,\n",
    "        cell_types\n",
    "    )\n",
    "    \n",
    "    for masks, boxes, cell, img_id, score in zip(\n",
    "        masks_pred, boxes_pred, cell_types, dataset.df['id'].values, scores\n",
    "    ):\n",
    "        metadata.append({\n",
    "            'id': img_id,\n",
    "            'rles': [rle_encode(mask) for mask in masks],\n",
    "            'boxes': boxes.tolist(),\n",
    "            'cell_type': cell,\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "\n",
    "    for i, s in enumerate(scores_per_class):\n",
    "        all_scores[i] += s\n",
    "        \n",
    "    del masks_pred, boxes_pred, cell_types\n",
    "    gc.collect()\n",
    "\n",
    "#     break\n",
    "    \n",
    "df_preds_oof = pd.DataFrame.from_dict(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(EXP_FOLDERS) == 1:\n",
    "    save_folder = EXP_FOLDERS[0]\n",
    "    name = 'df_oof.csv'\n",
    "else:\n",
    "    save_folder = OUT_PATH\n",
    "    name = 'df_oof_blend'\n",
    "\n",
    "    for f in EXP_FOLDERS:\n",
    "        name += f\"_{f.split('/')[-3][5:]}-{f.split('/')[-2]}\"\n",
    "    name += \".csv\"\n",
    "\n",
    "print(f'-> Saved results to \"{save_folder + name}\"')\n",
    "\n",
    "df_preds_oof.to_csv(save_folder + name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(len(all_scores)):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(CELL_TYPES[i], size=15)\n",
    "    plt.grid(True)\n",
    "    plt.scatter(range(len(all_scores[i])), sorted(all_scores[i]), s=20)\n",
    "#     plt.ylim(0, 0.75)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('IoU mAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.mean(np.concatenate(all_scores))\n",
    "scores_class = [np.mean(s) for s in all_scores if len(s)]\n",
    "\n",
    "print(f' -> IoU mAP : {score:.4f}\\n')\n",
    "\n",
    "for s, c in zip(scores_class, CELL_TYPES):\n",
    "    print(f'{c} : {s:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_preds, masks_truth = [], []\n",
    "\n",
    "for results, dataset in zip(all_results, datasets):\n",
    "    masks_pred, boxes_pred, cell_types = process_results(\n",
    "        results, best_thresholds_mask, best_thresholds_nms, best_thresholds_conf, remove_overlap=True\n",
    "    )\n",
    "    \n",
    "    masks_preds.append(masks.max(0))\n",
    "    \n",
    "    masks_truth = +[masks.masks.max(0) for masks in dataset.masks]\n",
    "\n",
    "dice_score(np.array(masks_preds), np.array(masks_truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SartoriusDataset(df_oof, transforms=pipelines['val_viz'], precompute_masks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in range(len(dataset)):\n",
    "    idx = 364\n",
    "    \n",
    "    score = df_preds_oof['score'][idx]\n",
    "    c = df_preds_oof['cell_type'][idx]\n",
    "    \n",
    "    data = dataset[idx]\n",
    "    img = data['img']\n",
    "    \n",
    "    # truth\n",
    "    truth = data['gt_masks'].masks.copy().astype(int)\n",
    "    boxes_truth = data['gt_bboxes']\n",
    "    \n",
    "    # preds\n",
    "    rles = df_preds_oof['rles'][idx]\n",
    "    pred = np.array([rle_decode(enc, ORIG_SIZE) for enc in rles]).astype(int)\n",
    "    boxes = df_preds_oof['boxes'][idx]\n",
    "    \n",
    "#     plt.figure(figsize=(15, 15))\n",
    "#     plot_sample(img, pred, boxes, plotly=False)\n",
    "#     plt.axis(False)\n",
    "#     plt.title(f'Pred - {CELL_TYPES[c]} - iou_map={score:.3f}')\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(15, 15))\n",
    "#     plot_sample(img, truth, boxes_truth, plotly=False)\n",
    "#     plt.axis(False)\n",
    "#     plt.title(f'Truth - {df_oof[\"cell_type\"][idx]}')\n",
    "#     plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plot_preds_iou(img, pred, truth, plot_tp=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=900,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single image explo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDERS = [\n",
    "    LOG_PATH + \"2021-11-16/0/\",  # r50 pretrain + extra - 0.3105\n",
    "#     LOG_PATH + \"2021-11-16/3/\",  # rx101 pretrain  - 0.3155\n",
    "#     LOG_PATH + \"2021-11-22/1/\",  # Cascade r50\n",
    "#     LOG_PATH + \"2021-11-22/5/\",  # Cascade rx101\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs, weights = [], []\n",
    "\n",
    "for exp_folder in EXP_FOLDERS:\n",
    "    config = Config(json.load(open(exp_folder + \"config.json\", 'r')))\n",
    "    config.model_config = exp_folder + config.model_config.split('/')[-1]\n",
    "    config.data_config = exp_folder + config.data_config.split('/')[-1]\n",
    "    config.split = \"skf\"\n",
    "    configs.append(config)\n",
    "\n",
    "#     weights.append(sorted(glob.glob(exp_folder + \"*.pt\")))\n",
    "    weights.append(sorted(glob.glob(exp_folder + \"*.pt\"))[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TTA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = prepare_data(fix=False)\n",
    "results_s, all_stuff, df_oof_s = inference_single(df, configs, weights, idx=0, use_tta=USE_TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmdet\n",
    "mmdet.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = define_pipelines(config.data_config)\n",
    "dataset_s = SartoriusDataset(df_oof_s, transforms=pipelines['val_viz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholds_mask = [0.45]\n",
    "thresholds_nms = [np.round(0.05 * i, 2) for i in range(1, 10)]\n",
    "thresholds_conf = [np.round(0.05 * i, 2) for i in range(1, 18)]\n",
    "\n",
    "scores = tweak_thresholds(\n",
    "    results_s,\n",
    "    dataset_s,\n",
    "    thresholds_mask,\n",
    "    thresholds_nms,\n",
    "    thresholds_conf,\n",
    "    remove_overlap=True\n",
    ")\n",
    "\n",
    "for c in range(len(CELL_TYPES)):\n",
    "    scores_class = scores[c]\n",
    "\n",
    "    if scores_class.shape[2]:\n",
    "        scores_class = scores[c].mean(2) \n",
    "        \n",
    "        idx = np.unravel_index(np.argmax(scores_class, axis=None), scores_class.shape)\n",
    "        best_score = scores_class[idx]\n",
    "        \n",
    "        threshold_mask = thresholds_mask[idx[0]]\n",
    "        threshold_nms = thresholds_nms[idx[1]]\n",
    "        threshold_conf = thresholds_conf[idx[2]]\n",
    "        \n",
    "        print(f\"Best score {best_score:.4f} for thresholds : \")\n",
    "        print(f'- Threshold mask : {threshold_mask}')\n",
    "        print(f'- Threshold nms  : {threshold_nms}')\n",
    "        print(f'- Threshold conf : {threshold_conf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold_mask = [0.45, 0.45, 0.45]\n",
    "# threshold_nms = [0.1, 0.1, 0.05]\n",
    "# threshold_conf = [0.3, 0.4, 0.7]\n",
    "\n",
    "masks_pred, boxes_pred, cell_types = process_results(\n",
    "    results_s, threshold_mask, threshold_nms, threshold_conf, remove_overlap=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_single, _ = evaluate(masks_pred, dataset_s, cell_types)\n",
    "\n",
    "print(f' -> IoU mAP : {np.mean(scores_single):.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "data = dataset_s[idx]\n",
    "\n",
    "img = data['img']\n",
    "truth = data['gt_masks'].masks.copy().astype(int)\n",
    "boxes_truth = data['gt_bboxes']\n",
    "pred = masks_pred[idx].copy().astype(int)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_sample(img, mask=pred, boxes=boxes_pred[idx])\n",
    "# plot_sample(img, mask=truth)\n",
    "plt.title(f'{CELL_TYPES[cell_types[idx]]} - iou_map={np.mean(scores_single):.3f}')\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    proposal_list, aug_proposals,\n",
    "    bboxes, merged_bboxes, aug_bboxes,\n",
    "    masks, merged_masks, aug_masks,\n",
    ") = all_stuff\n",
    "\n",
    "bboxes = bboxes.cpu().numpy()\n",
    "proposals = proposal_list[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Number of proposals : {[len(prop) for prop in aug_proposals[0]]}')\n",
    "print(f'Number of merged proposals : {len(proposals)}')\n",
    "print(f'Number of merged boxes : {len(merged_bboxes)}')\n",
    "print(f'Number of detected boxes (th=0.): {(bboxes[:, 4] > 0.).sum()}')\n",
    "print(f'Number of detected boxes (th=0.1): {(bboxes[:, 4] > 0.1).sum()}')\n",
    "print(f'Number of detected boxes (th=0.2): {(bboxes[:, 4] > 0.2).sum()}')\n",
    "print(f'Number of detected boxes (th=0.3): {(bboxes[:, 4] > 0.3).sum()}')\n",
    "print(f'Number of detected masks: {len(masks)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plot_sample(img, mask=None, boxes=proposals)\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_hit = 0.5\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "missed = []\n",
    "for i, preds in enumerate((proposals, bboxes)):\n",
    "    max_ious = []\n",
    "    for b in boxes_truth:\n",
    "        ious = []\n",
    "        for prop in preds[preds[:, 4] > 0.]:\n",
    "            ious.append(bbox_iou(b, prop))\n",
    "\n",
    "        max_ious.append(np.max(ious))\n",
    "\n",
    "    max_ious = np.array(max_ious)\n",
    "    missed.append(boxes_truth[(max_ious < threshold_hit)])\n",
    "\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    sns.histplot(max_ious, bins=20)\n",
    "    plt.axvline(threshold_hit, c=\"salmon\")\n",
    "    t = 'proposals' if i == 0 else \"bboxes\"\n",
    "    plt.title(t + f' - missed {len(missed[-1])}')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_preds_iou(\n",
    "    img,\n",
    "    pred,\n",
    "    truth,\n",
    "    boxes=missed[1],\n",
    "#     boxes_2=missed[1],\n",
    "    plot_tp=True)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=900,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
