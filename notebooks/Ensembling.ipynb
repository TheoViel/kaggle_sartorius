{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About** : This notebook is used to ensemble rle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import *\n",
    "\n",
    "from utils.plots import *\n",
    "from utils.metrics import *\n",
    "from utils.logger import Config\n",
    "from utils.rle import rle_encode, rle_decode\n",
    "\n",
    "from inference.tweaking import *\n",
    "from inference.validation import *\n",
    "from inference.post_process import *\n",
    "\n",
    "from data.preparation import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(False)\n",
    "df = df.sort_values('id').reset_index(drop=True)\n",
    "df = df[['id', 'annotation', 'cell_type', 'img_path', 'ann']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_FOLDER = LOG_PATH + \"2021-11-16/3/\"\n",
    "\n",
    "df_1 = pd.read_csv(EXP_FOLDER + \"df_oof.csv\")\n",
    "df_1 = df_1.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "df_1['rles'] = df_1['rles'].apply(ast.literal_eval)\n",
    "df_1['boxes'] = df_1['boxes'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_FOLDER = LOG_PATH + \"2021-11-22/5/\"\n",
    "\n",
    "# df_2 = pd.read_csv(EXP_FOLDER + \"df_oof.csv\")\n",
    "# df_2 = df_2.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# df_2['rles'] = df_2['rles'].apply(ast.literal_eval)\n",
    "# df_2['boxes'] = df_2['boxes'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stardist\n",
    "df_2 = pd.read_csv(OUT_PATH + \"preds_stardist_22_11.csv\")\n",
    "\n",
    "df_2 = df_2[\n",
    "    ['id', 'predicted_rles', 'probabilities', 'scores']\n",
    "].rename(columns={'scores': 'score', 'predicted_rles': 'rles', 'probabilities': 'boxes'})\n",
    "\n",
    "df_2['boxes'] = df_2['boxes'].apply(ast.literal_eval)\n",
    "df_2['boxes'] = df_2['boxes'].apply(lambda x: np.tile(np.array(x)[None], (6, 1)).T)\n",
    "df_2['rles'] = df_2['rles'].apply(ast.literal_eval)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    df_1, how='left', on=\"id\", suffixes=('', '_1')\n",
    ").rename(columns={'rles': 'rles_1', 'boxes': 'boxes_1', 'score': 'score_1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    df_2, how='left', on=\"id\", suffixes=('', '_2')\n",
    ").rename(columns={'rles': 'rles_2', 'boxes': 'boxes_2', 'score': 'score_2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfg = df[['cell_type', 'score_stardist', 'score_rcnn']].groupby('cell_type').agg(list)\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "\n",
    "# i = 1\n",
    "# for col in dfg.columns:\n",
    "#     for cell_type in dfg.index:\n",
    "#         scores = dfg[col][cell_type]\n",
    "#         plt.subplot(len(dfg.columns), len(dfg.index), i)\n",
    "#         plt.title(str(col + ' - ' + cell_type), size=15)\n",
    "#         plt.grid(True)\n",
    "#         plt.scatter(range(len(scores)), sorted(scores), s=20)\n",
    "#     #     plt.ylim(0, 0.75)\n",
    "#         plt.xlabel('')\n",
    "#         plt.ylabel('IoU mAP')\n",
    "#         i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = define_pipelines(\"data/config.py\")\n",
    "\n",
    "dataset = SartoriusDataset(df, transforms=pipelines['val_viz'], precompute_masks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_merge_preds(pred_1, pred_2, truth):\n",
    "    \n",
    "    rles_2 = [pycocotools.mask.encode(np.asarray(p, order='F')) for p in pred_2.astype(np.uint8)]\n",
    "    rles_1 = [pycocotools.mask.encode(np.asarray(p, order='F')) for p in pred_1.astype(np.uint8)]\n",
    "\n",
    "    ious = pycocotools.mask.iou(rles_1, rles_2, [0] * 100000)\n",
    "    ious_sum = ious.sum(0)\n",
    "    \n",
    "    all_rles = rles_1 + [p for i, p in enumerate(rles_2) if ious_sum[i] <= 0.]\n",
    "    \n",
    "    iou = pycocotools.mask.iou(truth, all_rles, [0] * 100000)\n",
    "    score = iou_map(ious=[iou])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_1, scores_per_class_1 = evaluate(masks_1, dataset, df['cell_type'].apply(lambda x: CELL_TYPES.index(x)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = np.mean(np.concatenate(scores_per_class_1))\n",
    "# scores_class = [np.mean(s) if len(s) else 0 for s in scores_per_class_1]\n",
    "\n",
    "# print(f' -> IoU mAP : {score:.4f}\\n')\n",
    "\n",
    "# for s, c in zip(scores_class, CELL_TYPES):\n",
    "#     print(f'{c} : {s:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_2, scores_per_class_2 = evaluate(masks_2, dataset, df['cell_type'].apply(lambda x: CELL_TYPES.index(x)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = np.mean(np.concatenate(scores_per_class_2))\n",
    "# scores_class = [np.mean(s) if len(s) else 0 for s in scores_per_class_2]\n",
    "\n",
    "# print(f' -> IoU mAP : {score:.4f}\\n')\n",
    "\n",
    "# for s, c in zip(scores_class, CELL_TYPES):\n",
    "#     print(f'{c} : {s:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(f' -> IoU mAP : {np.mean(scores_merge):.4f}\\n')\n",
    "\n",
    "# for idx in range(3):\n",
    "#     s = np.mean([scores_merge[i] for i, c in enumerate(df['cell_type']) if c == CELL_TYPES[idx]])\n",
    "#     print(f'{CELL_TYPES[idx]} : {s:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_merged = []\n",
    "\n",
    "for i in tqdm(range(len(dataset))):\n",
    "    # 1\n",
    "    rles_1 = df['rles_1'][i]\n",
    "    pred_1 = np.array([rle_decode(enc, ORIG_SIZE) for enc in rles_1])\n",
    "    \n",
    "    # 2\n",
    "    rles_2 = df['rles_2'][i]\n",
    "    pred_2 = np.array([rle_decode(enc, ORIG_SIZE) for enc in rles_2])\n",
    "    \n",
    "    mask_to_merge = np.concatenate([pred_1, pred_2], 0)\n",
    "    \n",
    "    b2 = np.zeros(df_2['boxes'][i].shape)\n",
    "    boxes_to_merge = np.concatenate([df_1['boxes'][i], b2], 0)\n",
    "    \n",
    "    masks, boxes, picks = mask_nms(mask_to_merge, boxes_to_merge, 0.00000001)\n",
    "    \n",
    "    masks = remove_overlap_naive(masks)\n",
    "    \n",
    "    masks_merged.append(masks)    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_m, scores_per_class_m = evaluate(masks_merged, dataset, df['cell_type'].apply(lambda x: CELL_TYPES.index(x)).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.mean(np.concatenate(scores_per_class_m))\n",
    "scores_class = [np.mean(s) if len(s) else 0 for s in scores_per_class_m]\n",
    "\n",
    "print(f' -> IoU mAP : {score:.4f}\\n')\n",
    "\n",
    "for s, c in zip(scores_class, CELL_TYPES):\n",
    "    print(f'{c} : {s:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score_1'][i], df['score_2'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in tqdm(range(len(dataset))):\n",
    "    score_1 = df['score_1'][idx]\n",
    "    cell_1 = df['cell_type_1'][idx]\n",
    "\n",
    "    score_2 = df['score_2'][idx]\n",
    "    try:\n",
    "        cell_2 = df['cell_type_2'][idx]\n",
    "    except:\n",
    "        cell_2 = 0\n",
    "    \n",
    "    # 1\n",
    "    rles_1 = df['rles_1'][idx]\n",
    "    pred_1 = np.array([rle_decode(enc, ORIG_SIZE) for enc in rles_1]).astype(int)\n",
    "    \n",
    "    # 2\n",
    "    rles_2 = df['rles_2'][idx]\n",
    "    pred_2 = np.array([rle_decode(enc, ORIG_SIZE) for enc in rles_2]).astype(int)\n",
    "    \n",
    "    # merged\n",
    "    mask_m = masks_merged[idx].astype(int).copy()\n",
    "    \n",
    "    # plot\n",
    "    data = dataset[idx]\n",
    "    img = data['img']\n",
    "    truth = data['gt_masks'].masks.copy().astype(int)\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plot_sample(img, pred_1.copy())\n",
    "    plt.axis(False)\n",
    "    plt.title(f'Pred 1 - {CELL_TYPES[cell_1]} - iou_map={score_1:.3f}')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plot_sample(img, pred_2.copy())\n",
    "    plt.axis(False)\n",
    "    plt.title(f'Pred 2 - {CELL_TYPES[cell_2]} - iou_map={score_2:.3f}')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plot_sample(img, mask_m)\n",
    "    plt.axis(False)\n",
    "    plt.title(f'Pred merged - {CELL_TYPES[cell_1]} - iou_map={0:.3f}')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plot_sample(img, truth.copy(), plotly=False)\n",
    "    plt.axis(False)\n",
    "    plt.title(f'Truth - {df[\"cell_type\"][idx]}')\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plot_preds_iou(img, pred_1, mask_m, plot_tp=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=900,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plot_preds_iou(img, mask_m, truth, plot_tp=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=900,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plot_preds_iou(img, pred_2, mask_m, plot_tp=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=900,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plot_preds_iou(img, mask_m, truth, plot_tp=False)\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=900,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
